{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVhils5DlfU4",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing: Twitter Classification\n",
        "The problem being solved is being able to detect whether a tweet is relevant or irrelevant to a disaster. Many times, people who tweet make references to disaster to describe but aren't referring to a real disaster. \n",
        "\n",
        "DATASET: http://bit.ly/Twitter_Dataset\n",
        "\n",
        "Rather than use a neural network, we used a Naive Bayes algorithm, specifically Bernoulli which is a specialized form of logarithmic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqKLbsE0okKB",
        "colab_type": "text"
      },
      "source": [
        "#Twitter Training Dataset\n",
        "\n",
        "\n",
        "1.   Number of Instances: 7416 instances\n",
        "2.   Number of Attributes: 2 plus the class attribute\n",
        "3.   Attribute Information: Attribute Domain\n",
        "\n",
        "      1. Index of tweet\n",
        "      2. Class: 0 for irrelevant, 1 for relevant\n",
        "      3. Text: Text of tweet\n",
        "      \n",
        "4. Missing Attribute Value: N/A\n",
        "5. Class Distribution:\n",
        "\n",
        "  Instance belonging to class 0: 4,305 (58.1%)\n",
        "  \n",
        "  Instances belonging to class 1: 3,111 (41.9%)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9J3AO8kGL3q",
        "colab_type": "code",
        "outputId": "33213a4b-f3d2-481b-fc6f-6cd773498bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Imports\n",
        "            \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras  \n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import BernoulliNB\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0JZ-gkvGetx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Collection\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df2 = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Take relevant data from DataFrame and put it in Numpy Array\n",
        "\n",
        "tweets = df['text'].tolist()\n",
        "labels = df['class_label'].to_numpy()\n",
        "testTweets = df2['text'].tolist()\n",
        "testLabels = df2['class_label'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqHE0zBnrT-L",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing\n",
        "We created a lemmatizing method and defined a list of stopwords to remove. We use a vectorizer with the defined a preprocessing method and stopwords to convert all the tweets into a large vector. Each represents "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RTOf5-UqMRC",
        "colab_type": "code",
        "outputId": "e84be040-f928-402f-8441-084010773ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# TFID Vectorizer\n",
        "def preprocess(s):\n",
        "  lemmatizer = nltk.WordNetLemmatizer()\n",
        "  return lemmatizer.lemmatize(s)\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop, analyzer='word', max_features=20000, dtype=np.float32, preprocessor=preprocess)\n",
        "\n",
        "data = vectorizer.fit_transform(tweets).toarray()\n",
        "testData = vectorizer.transform(testTweets).toarray()\n",
        "print(type(data), data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uihXxOg5Glv5",
        "colab_type": "code",
        "outputId": "8552a71a-3971-4309-e004-cbfbd03ac3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.3, random_state=10)\n",
        "X_train = data\n",
        "Y_train = labels\n",
        "X_test = testData\n",
        "Y_test = testLabels\n",
        "\n",
        "print(X_test)\n",
        "print(Y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHuU2fAGp8K",
        "colab_type": "code",
        "outputId": "4c868c8d-891b-46dc-d04d-67d298247aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "clf = BernoulliNB()\n",
        "clf.fit(X_train, Y_train)\n",
        "BernoulliNB(alpha=2.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "print(clf.score(X_test, Y_test))\n",
        "\n",
        "print(recall_score(Y_test, predictions, average='macro'))\n",
        "print(precision_score(Y_test, predictions, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n",
            "0.8058252427184466\n",
            "0.7808352143656005\n",
            "0.8184909531575053\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}